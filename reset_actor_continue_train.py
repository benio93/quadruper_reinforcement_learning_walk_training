import gym
import torch
from stable_baselines3 import PPO
from base_walk_env import BaseWalkEnv

# ğŸ”„ --- 1. Wczytaj stary model -----------------------------------------
old_model_path = "models/ppo_zero_Z26.zip"

old_model = PPO.load(old_model_path)

# ğŸ” --- 2. Przygotuj Å›rodowisko ----------------------------------------
env = BaseWalkEnv()

# âš™ï¸ --- 3. StwÃ³rz nowy model, z wiÄ™kszÄ… entropiÄ… ------------------------ 
# UWAGA: podbijam ent_coef tutaj!
new_model = PPO(
    "MlpPolicy",
    env,
    verbose=1,
    ent_coef=0.002,      # <-- tu dodajemy entropiÄ™
    learning_rate=3e-4, # moÅ¼esz teÅ¼ lekko obniÅ¼yÄ‡ LR
    clip_range=0.2      # lub zostawiÄ‡ 0.2, ale mniejsze klipy bywajÄ… stabilniejsze
)

# ğŸ”§ --- 4. Skopiuj krytyka ----------------------------------------------
with torch.no_grad():
    new_model.policy.value_net.load_state_dict(
        old_model.policy.value_net.state_dict()
    )

# ğŸ†• --- 5. Trenuj dalej z nowym aktorem i starym krytykiem ---------------
new_model.learn(total_timesteps=100_000)






# ğŸ’¾ --- 6. Zapisz nowy model --------------------------------------------
new_model.save("models/new_z_entropy.zip")

print("âœ… Zapisano zresetowany aktor + stary krytyk -> models/ppo_reset_actor.zip")
