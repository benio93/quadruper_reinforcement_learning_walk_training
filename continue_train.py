import argparse, os, glob, csv
from stable_baselines3 import PPO
from base_walk_env import BaseWalkEnv
from stable_baselines3.common.callbacks import BaseCallback

class InfoLoggerCallback(BaseCallback):
    def __init__(self, log_dir="logs", keep_last=5, verbose=0):
        super().__init__(verbose)
        os.makedirs(log_dir, exist_ok=True)
        existing = sorted(glob.glob(os.path.join(log_dir, "metrics_run_*.csv")))
        run_id = len(existing) + 1
        self.log_path = os.path.join(log_dir, f"metrics_run_{run_id:03d}.csv")
        if len(existing) >= keep_last:
            for pth in existing[:len(existing) - keep_last + 1]:
                os.remove(pth)
        self.file = open(self.log_path, "w", newline="")
        self.writer = csv.writer(self.file)
        header = [
            "step", "progress", "streak",
            "fall_count", "yaw_limit_count", "lateral_limit_count", "too_high_count",
            "tilt_count", "max_steps_count", "crossed_finish_line_count",
            "r_forward_sum", "r_contact_sum", "r_gait_sum", "r_smooth_sum",
            "r_terminal_sum", "r_lat_soft_sum",
            "r_alive_sum", "r_time_sum", "r_lateral_sum", "r_rotation_sum",
            "r_distance_sum", "r_yaw_soft_sum", "r_air_sum",
            "r_joint_avg_sum", "r_joint_full_sum"
        ]
        self.writer.writerow(header)
        self.succ = self.fail = self.streak = 0
        self.fall_count = self.yaw_limit_count = self.lateral_limit_count = 0
        self.too_high_count = self.tilt_count = 0
        self.max_steps_count = self.crossed_finish_line_count = 0
        self.r_forward_sum = self.r_contact_sum = self.r_gait_sum = 0.0
        self.r_smooth_sum = self.r_terminal_sum = self.r_lat_soft_sum = 0.0
        self.r_alive_sum = self.r_time_sum = self.r_lateral_sum = self.r_rotation_sum = 0.0
        self.r_distance_sum = self.r_yaw_soft_sum = self.r_air_sum = 0.0
        self.r_joint_avg_sum = self.r_joint_full_sum = 0.0

    def _on_step(self) -> bool:
        for r, info in zip(self.locals["rewards"], self.locals["infos"]):
            reason = info.get("done_reason")
            self.r_forward_sum      += info.get("r_forward", 0.0)
            self.r_contact_sum      += info.get("r_contact", 0.0)
            self.r_gait_sum         += info.get("r_gait", 0.0)
            self.r_smooth_sum       += info.get("r_smooth", 0.0)
            self.r_terminal_sum     += info.get("r_terminal", 0.0)
            self.r_lat_soft_sum     += info.get("r_lat_soft", 0.0)
            self.r_alive_sum        += info.get("r_alive", 0.0)
            self.r_time_sum         += info.get("r_time", 0.0)
            self.r_lateral_sum      += info.get("r_lateral", 0.0)
            self.r_rotation_sum     += info.get("r_rotation", 0.0)
            self.r_distance_sum     += info.get("r_distance", 0.0)
            self.r_yaw_soft_sum     += info.get("r_yaw_soft", 0.0)
            self.r_air_sum          += info.get("r_air", 0.0)
            self.r_joint_avg_sum    += info.get("r_joint_avg", 0.0)
            self.r_joint_full_sum   += info.get("r_joint_full", 0.0)

            if reason is not None:
                if reason in ("crossed_finish_line", "target_reached"):
                    self.crossed_finish_line_count += 1
                    self.succ += 1
                    self.streak += 1
                elif reason == "max_steps":
                    self.max_steps_count += 1
                    self.succ += 1
                    self.streak += 1
                else:
                    self.fail += 1
                    self.streak = 0
                    if reason == "fall":
                        self.fall_count += 1
                    elif reason == "yaw_limit":
                        self.yaw_limit_count += 1
                    elif reason == "lateral_limit":
                        self.lateral_limit_count += 1
                    elif reason == "too_high":
                        self.too_high_count += 1
                    elif reason == "tilt":
                        self.tilt_count += 1

                self.writer.writerow([
                    self.num_timesteps,
                    info.get("progress_to_target", 0.0),
                    self.streak,
                    self.fall_count,
                    self.yaw_limit_count,
                    self.lateral_limit_count,
                    self.too_high_count,
                    self.tilt_count,
                    self.max_steps_count,
                    self.crossed_finish_line_count,
                    self.r_forward_sum,
                    self.r_contact_sum,
                    self.r_gait_sum,
                    self.r_smooth_sum,
                    self.r_terminal_sum,
                    self.r_lat_soft_sum,
                    self.r_alive_sum,
                    self.r_time_sum,
                    self.r_lateral_sum,
                    self.r_rotation_sum,
                    self.r_distance_sum,
                    self.r_yaw_soft_sum,
                    self.r_air_sum,
                    self.r_joint_avg_sum,
                    self.r_joint_full_sum
                ])

                self.r_forward_sum = self.r_contact_sum = self.r_gait_sum = 0.0
                self.r_smooth_sum = self.r_terminal_sum = self.r_lat_soft_sum = 0.0
                self.r_alive_sum = self.r_time_sum = self.r_lateral_sum = self.r_rotation_sum = 0.0
                self.r_distance_sum = self.r_yaw_soft_sum = self.r_air_sum = 0.0
                self.r_joint_avg_sum = self.r_joint_full_sum = 0.0

        return True

    def _on_training_end(self):
        self.file.close()


def main(model_path: str, more_steps: int):
    # --- 1. Inicjalizacja Å›rodowiska ---
    env = BaseWalkEnv()

    # --- 2. PARAMETRY DO MANIPULACJI (Identyczne jak w Base Train) ---
    # MoÅ¼esz tutaj zmieniaÄ‡ wartoÅ›ci, aby oduczyÄ‡ robota szurania lub drÅ¼enia.
    
    new_params = {
     #   "learning_rate": 0.0003,    # SzybkoÅ›Ä‡ nauki. Zmniejsz (np. 0.00005), aby wygÅ‚adziÄ‡ ruchy pod koniec.
        
    
    
    
    
     #   "n_steps": 2048,           # Liczba krokÃ³w przed aktualizacjÄ…. Musi byÄ‡ taka sama jak w oryginale, by nie byÅ‚o bÅ‚Ä™dÃ³w pamiÄ™ci.
        
     #   "batch_size": 64,          # WielkoÅ›Ä‡ paczki danych.
        
     #   "n_epochs": 10,            # Ile razy sieÄ‡ "mieli" te same dane.
        
     #   "gamma": 0.99,             # Znaczenie nagrÃ³d dÅ‚ugoterminowych.
        
     #   "gae_lambda": 0.95,        # WygÅ‚adzanie szacunkÃ³w nagrody.
        
     #   "clip_range": 0.2,         # Zakres dopuszczalnych zmian w polityce. 0.1 to ruchy bardziej konserwatywne.
        
     #   "ent_coef": 0.02,          # EKSPLORACJA (Na szuranie). ZwiÄ™ksz do 0.05, jeÅ›li robot nie prÃ³buje podnosiÄ‡ nÃ³g.
        
     #   "vf_coef": 0.5,            # Waga bÅ‚Ä™du funkcji wartoÅ›ci.
        
     #   "max_grad_norm": 0.5       # Maksymalna norma gradientu.


            # 1. ZWIÄ˜KSZ ENTROPIÄ˜ (Chaos)
        # To najwaÅ¼niejsza zmiana. Robot musi zaczÄ…Ä‡ znowu "drgaÄ‡" i prÃ³bowaÄ‡ rÃ³Å¼nych ruchÃ³w.
        "ent_coef": 0.005,          # Z 0.02 na 0.05 â€“ to zmusi go do ponownej eksploracji.

        # 2. ZMNIEJSZ LEARNING RATE (PrÄ™dkoÅ›Ä‡ nauki)
        # Skoro robot juÅ¼ umie ustaÄ‡, nie chcemy, Å¼eby nagÅ‚y chaos zniszczyÅ‚ tÄ™ umiejÄ™tnoÅ›Ä‡.
        # Mniejszy LR pozwoli mu powoli nadpisywaÄ‡ "stanie" nowym "chodzeniem".
        "learning_rate": 0.0001,   # Z 0.0003 na 0.0001.

        # 3. ZWIÄ˜KSZ GAMMA (DalekowzrocznoÅ›Ä‡)
        # Robot musi bardziej chcieÄ‡ nagrody za "progress", ktÃ³ra jest daleko, 
        # niÅ¼ przejmowaÄ‡ siÄ™ maÅ‚Ä… karÄ… za chwilowe zachwianie.
        "gamma": 0.995,            # Z 0.99 na 0.999.

        # 4. ZWIÄ˜KSZ CLIP RANGE
        # PozwÃ³lmy modelowi na nieco wiÄ™ksze jednorazowe zmiany w zachowaniu.
        "clip_range": 0.10,         # Z 0.2 na 0.3.

        # Reszta zostaje bez zmian, aby nie wywaliÄ‡ bÅ‚Ä™dÃ³w bufora:
        "n_steps": 2048,
        "batch_size": 64,
        "n_epochs": 5,
        "gae_lambda": 0.95,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
    }

    print(f"ğŸ”„ Wznawiam trening modelu: {model_path}")
    print(f"âš™ï¸ StosujÄ™ parametry: {new_params}")
    
    # --- 3. Åadowanie modelu z nadpisaniem parametrÃ³w ---
    try:
        model = PPO.load(
            model_path, 
            env=env, 
            custom_objects=new_params, # Wstrzykujemy nasze parametry
            device="auto"              # Automatyczny wybÃ³r CPU/GPU
        )
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d podczas Å‚adowania modelu: {e}")
        return

    # --- 4. Kontynuacja nauki ---
    model.learn(
        total_timesteps=more_steps,
        reset_num_timesteps=False,   # Kontynuujemy licznik (nie zerujemy krokÃ³w w Tensorboard)
        callback=InfoLoggerCallback("logs")
    )

    # --- 5. Zapis zaktualizowanego modelu ---
    new_save_path = "models/alfa_19.zip"

    os.makedirs("models", exist_ok=True)
    model.save(new_save_path)
    print(f"ğŸ’¾ Sukces! Zaktualizowany model zapisany jako: {new_save_path}")
    env.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Kontynuacja treningu Quadrobota")
    parser.add_argument("--model_path", type=str, default="models/alfa_18.zip", help="ÅšcieÅ¼ka do pliku .zip modelu")
    parser.add_argument("--timesteps", type=int, default=100000, help="Liczba dodatkowych krokÃ³w do wytrenowania")
    
    args = parser.parse_args()

    # Sprawdzenie czy plik istnieje przed startem
    if not os.path.exists(args.model_path):
        print(f"âš ï¸ Nie znaleziono modelu w Å›cieÅ¼ce: {args.model_path}")
    else:
        main(args.model_path, args.timesteps)